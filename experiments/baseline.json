{
    "batch_size": 32,
    "n_epoch": 7,
    "T": 2,
    "model": {
        "num_hidden_layers": 16,
        "num_attention_heads": 8,
        "hidden_size": 512,
        "intermediate_size": 2048
    },
    "optimizer": {
        "lr": 0.00025,
        "weight_decay": 0.01
    }
}